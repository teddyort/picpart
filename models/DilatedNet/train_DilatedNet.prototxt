layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  python_param {
    module: "ade_layers"
    layer: "AdeSegDataLayer"
    param_str: "{\'seed\': 1337, \'randomize\': True, \'split\': \'training\', \'mean\': (109.5388, 118.6897, 124.6901), \'batch_size\': 1, \'phase\': \'TRAIN\', \'fine_size\': 384, \'resize_mode\': \'scale\', \'loader\': \'disk\'}"
  }
  include {
    phase: TRAIN
  }
}

layer {
  name: "data"
  type: "Python"
  top: "data"
  top: "label"
  python_param {
    module: "ade_layers"
    layer: "AdeSegDataLayer"
    param_str: "{\'seed\': 1337, \'randomize\': False, \'split\': \'validation\', \'mean\': (109.5388, 118.6897, 124.6901), \'batch_size\': 1, \'phase\': \'TEST\', \'fine_size\': 384, \'resize_mode\': \'scale\', \'loader\': \'disk\'}"
  }
  include {
    phase: TEST
  }
}

layer {
  name: "conv1_1"
  type: "Convolution"
  bottom: "data"
  top: "conv1_1"
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu1_1"
  type: "ReLU"
  bottom: "conv1_1"
  top: "conv1_1"
}
layer {
  name: "conv1_2"
  type: "Convolution"
  bottom: "conv1_1"
  top: "conv1_2"
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu1_2"
  type: "ReLU"
  bottom: "conv1_2"
  top: "conv1_2"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1_2"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_1"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2_1"
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu2_1"
  type: "ReLU"
  bottom: "conv2_1"
  top: "conv2_1"
}
layer {
  name: "conv2_2"
  type: "Convolution"
  bottom: "conv2_1"
  top: "conv2_2"
  convolution_param {
    num_output: 128
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu2_2"
  type: "ReLU"
  bottom: "conv2_2"
  top: "conv2_2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2_2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv3_1"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3_1"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu3_1"
  type: "ReLU"
  bottom: "conv3_1"
  top: "conv3_1"
}
layer {
  name: "conv3_2"
  type: "Convolution"
  bottom: "conv3_1"
  top: "conv3_2"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu3_2"
  type: "ReLU"
  bottom: "conv3_2"
  top: "conv3_2"
}
layer {
  name: "conv3_3"
  type: "Convolution"
  bottom: "conv3_2"
  top: "conv3_3"
  convolution_param {
    num_output: 256
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu3_3"
  type: "ReLU"
  bottom: "conv3_3"
  top: "conv3_3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3_3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv4_1"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu4_1"
  type: "ReLU"
  bottom: "conv4_1"
  top: "conv4_1"
}
layer {
  name: "conv4_2"
  type: "Convolution"
  bottom: "conv4_1"
  top: "conv4_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu4_2"
  type: "ReLU"
  bottom: "conv4_2"
  top: "conv4_2"
}
layer {
  name: "conv4_3"
  type: "Convolution"
  bottom: "conv4_2"
  top: "conv4_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    pad: 1
  }
}
layer {
  name: "relu4_3"
  type: "ReLU"
  bottom: "conv4_3"
  top: "conv4_3"
}
layer {
  name: "conv5_1"
  type: "Convolution"
  bottom: "conv4_3"
  top: "conv5_1"
  convolution_param {
    num_output: 512
    kernel_size: 3
    dilation: 2
    pad: 2
  }
}
layer {
  name: "relu5_1"
  type: "ReLU"
  bottom: "conv5_1"
  top: "conv5_1"
}
layer {
  name: "conv5_2"
  type: "Convolution"
  bottom: "conv5_1"
  top: "conv5_2"
  convolution_param {
    num_output: 512
    kernel_size: 3
    dilation: 2
    pad: 2
  }
}
layer {
  name: "relu5_2"
  type: "ReLU"
  bottom: "conv5_2"
  top: "conv5_2"
}
layer {
  name: "conv5_3"
  type: "Convolution"
  bottom: "conv5_2"
  top: "conv5_3"
  convolution_param {
    num_output: 512
    kernel_size: 3
    dilation: 2
    pad: 2
  }
}
layer {
  name: "relu5_3"
  type: "ReLU"
  bottom: "conv5_3"
  top: "conv5_3"
}
layer {
  name: "fc6"
  type: "Convolution"
  bottom: "conv5_3"
  top: "fc6"
  convolution_param {
    num_output: 4096
    kernel_size: 7
    dilation: 4
    pad: 12
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "Convolution"
  bottom: "fc6"
  top: "fc7"
  convolution_param {
    num_output: 4096
    kernel_size: 1
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc-final151"
  type: "Convolution"
  bottom: "fc7"
  top: "fc_final"
  convolution_param {
    num_output: 151
    kernel_size: 1
  }
}
layer {
  name: "upsample"
  type: "Deconvolution"
  bottom: "fc_final" 
  top: "fc_final_up"
  convolution_param {
    kernel_size: 16
    stride: 8
    num_output: 151
    group: 151
    pad: 4
    weight_filler: { type: "bilinear" } 
    bias_term: false
  }
  param { lr_mult: 0 decay_mult: 0 }
}

layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc_final_up"
  bottom: "label"
  top: "loss"
  softmax_param {engine: CAFFE}
  loss_param {
    weight_by_label_freqs: true
    ignore_label: 0
    class_weighting: 0      # 0
    class_weighting: 0.038  # 1
    class_weighting: 0.074  # 2
    class_weighting: 0.054  # 3
    class_weighting: 0.048  # 4
    class_weighting: 0.067  # 5
    class_weighting: 0.068  # 6
    class_weighting: 0.111  # 7
    class_weighting: 0.234  # 8
    class_weighting: 0.095  # 9
    class_weighting: 0.184  # 10
    class_weighting: 0.155  # 11
    class_weighting: 0.145  # 12
    class_weighting: 0.088  # 13
    class_weighting: 0.247  # 14
    class_weighting: 0.067  # 15
    class_weighting: 0.104  # 16
    class_weighting: 0.264  # 17
    class_weighting: 0.112  # 18
    class_weighting: 0.208  # 19
    class_weighting: 0.137  # 20
    class_weighting: 0.141  # 21
    class_weighting: 0.629  # 22
    class_weighting: 0.135  # 23
    class_weighting: 0.374  # 24
    class_weighting: 0.294  # 25
    class_weighting: 0.669  # 26
    class_weighting: 0.685  # 27
    class_weighting: 0.241  # 28
    class_weighting: 0.385  # 29
    class_weighting: 0.929  # 30
    class_weighting: 0.381  # 31
    class_weighting: 0.345  # 32
    class_weighting: 0.322  # 33
    class_weighting: 0.639  # 34
    class_weighting: 0.571  # 35
    class_weighting: 1.174  # 36
    class_weighting: 0.144  # 37
    class_weighting: 1.104  # 38
    class_weighting: 0.555  # 39
    class_weighting: 0.307  # 40
    class_weighting: 1.085  # 41
    class_weighting: 0.310  # 42
    class_weighting: 0.557  # 43
    class_weighting: 0.168  # 44
    class_weighting: 0.812  # 45
    class_weighting: 1.215  # 46
    class_weighting: 1.434  # 47
    class_weighting: 0.378  # 48
    class_weighting: 1.554  # 49
    class_weighting: 0.953  # 50
    class_weighting: 1.109  # 51
    class_weighting: 3.431  # 52
    class_weighting: 0.795  # 53
    class_weighting: 0.507  # 54
    class_weighting: 5.186  # 55
    class_weighting: 2.593  # 56
    class_weighting: 2.253  # 57
    class_weighting: 0.480  # 58
    class_weighting: 3.209  # 59
    class_weighting: 0.791  # 60
    class_weighting: 1.394  # 61
    class_weighting: 1.709  # 62
    class_weighting: 1.622  # 63
    class_weighting: 1.331  # 64
    class_weighting: 0.563  # 65
    class_weighting: 1.129  # 66
    class_weighting: 0.341  # 67
    class_weighting: 0.401  # 68
    class_weighting: 1.677  # 69
    class_weighting: 0.677  # 70
    class_weighting: 1.347  # 71
    class_weighting: 0.840  # 72
    class_weighting: 1.209  # 73
    class_weighting: 3.097  # 74
    class_weighting: 1.683  # 75
    class_weighting: 1.377  # 76
    class_weighting: 1.467  # 77
    class_weighting: 2.624  # 78
    class_weighting: 6.559  # 79
    class_weighting: 6.862  # 80
    class_weighting: 1.798  # 81
    class_weighting: 0.907  # 82
    class_weighting: 0.178  # 83
    class_weighting: 1.014  # 84
    class_weighting: 3.034  # 85
    class_weighting: 0.765  # 86
    class_weighting: 0.837  # 87
    class_weighting: 0.224  # 88
    class_weighting: 6.282  # 89
    class_weighting: 0.722  # 90
    class_weighting: 3.304  # 91
    class_weighting: 5.373  # 92
    class_weighting: 2.506  # 93
    class_weighting: 0.445  # 94
    class_weighting: 2.451  # 95
    class_weighting: 0.987  # 96
    class_weighting: 10.619 # 97
    class_weighting: 1.453  # 98
    class_weighting: 0.462  # 99
    class_weighting: 3.812  # 100
    class_weighting: 1.260  # 101
    class_weighting: 4.130  # 102
    class_weighting: 0.801  # 103
    class_weighting: 8.577  # 104
    class_weighting: 4.505  # 105
    class_weighting: 7.825  # 106
    class_weighting: 1.527  # 107
    class_weighting: 5.792  # 108
    class_weighting: 1.312  # 109
    class_weighting: 6.758  # 110
    class_weighting: 0.959  # 111
    class_weighting: 8.920  # 112
    class_weighting: 0.717  # 113
    class_weighting: 5.575  # 114
    class_weighting: 7.559  # 115
    class_weighting: 0.840  # 116
    class_weighting: 1.582  # 117
    class_weighting: 6.110  # 118
    class_weighting: 1.025  # 119
    class_weighting: 3.279  # 120
    class_weighting: 3.845  # 121
    class_weighting: 1.677  # 122
    class_weighting: 7.690  # 123
    class_weighting: 1.067  # 124
    class_weighting: 1.398  # 125
    class_weighting: 0.374  # 126
    class_weighting: 4.598  # 127
    class_weighting: 1.285  # 128
    class_weighting: 8.577  # 129
    class_weighting: 1.813  # 130
    class_weighting: 4.130  # 131
    class_weighting: 2.219  # 132
    class_weighting: 1.565  # 133
    class_weighting: 1.664  # 134
    class_weighting: 0.437  # 135
    class_weighting: 0.348  # 136
    class_weighting: 0.845  # 137
    class_weighting: 0.985  # 138
    class_weighting: 0.665  # 139
    class_weighting: 1.123  # 140
    class_weighting: 4.848  # 141
    class_weighting: 1.956  # 142
    class_weighting: 0.782  # 143
    class_weighting: 2.055  # 144
    class_weighting: 2.165  # 145
    class_weighting: 3.431  # 146
    class_weighting: 2.506  # 147
    class_weighting: 0.885  # 148
    class_weighting: 0.575  # 149
    class_weighting: 1.059  # 150
  }
}

layer {
  name: "SegAccuracy"
  type: "Python"
  bottom: "fc_final_up"
  bottom: "label"
  top: "seg_accuracy"
  python_param {
    module: "accuracy_layer"
    layer: "SegmentationAccuracy"
    param_str: "{\'ignore_label\': 0, \'verbose\': True}"
  }
  include {
    phase: TEST
  }
}

layer {
  name: "IoU"
  type: "Python"
  bottom: "fc_final_up"
  bottom: "label"
  top: "meanIoU"
  python_param {
    module: "IoU_layer"
    layer: "IoULayer"
    param_str: "{\'classes\': 151, \'verbose\': True}"
  }
}

layer {
  name: "Accuracy"
  type: "Accuracy"
  bottom: "fc_final_up"
  bottom: "label"
  top: "accuracy"
  accuracy_param {
    top_k: 1
    ignore_label: 0
    axis: 1
  }
}

layer {
  name: "Logger"
  type: "Python"
  bottom: "accuracy"
  bottom: "meanIoU"
  bottom: "loss"
  python_param {
    module: "log_layer"
    layer: "LogLayer"
    param_str: "{\'file\':\'DilatedNet_train_log_Dec08_1115.txt\',\'headers\': (\'accuracy\',\'meanIoU\',\'loss\')}"
  }
  include {
    phase: TRAIN
  }
}
layer {
  name: "Logger"
  type: "Python"
  bottom: "accuracy"
  bottom: "meanIoU"
  bottom: "loss"
  python_param {
    module: "log_layer"
    layer: "LogLayer"
    param_str: "{\'file\':\'DilatedNet_test_log_Dec08_1115.txt\',\'headers\': (\'accuracy\',\'meanIoU\',\'loss\')}"
  }
  include {
    phase: TEST
  }
}
